---
title: "Defer Sender Algorithm Customization to Post-C++26"
document: D3826R0
date: today
audience:
  - "SG1 Concurrency and Parallelism Working Group"
  - "LEWG Library Evolution Working Group"
  - "LWG Library Working Group"
author:
  - name: Eric Niebler
    email: <eric.niebler@gmail.com>
toc: true
---


# Background

In the current Working Draft, [exec]{.sref} has sender algorithms that are customizable.
While the sender/receiver concepts and the algorithms themselves have been stable for
several years now, the customization mechanism has seen a fair bit of recent churn.
[@P3718R0] is the latest effort to shore up the mechanism. Unfortunately, there are gaps
in its proposed resolution. This paper details those gaps.

The problems are fixable although the fixes are non-trivial. The time for elaborate fixes
has passed. This paper proposes to remove the ability to customize sender algorithms for
C++26. A future paper will propose to add the feature back post-'26.

The author feels that postponing the feature (details below) will be a less disruptive and
safer change than trying to patch it at the last minute. Most common usages of
sender/receiver will not be affected -- algorithm customization is useful largely for
non-CPU-ish execution resources like hardware accelerators. Resources like thread pools,
message loops, async IO subsystems, and networking will not be impacted by this change.

# Downsides of delaying sender alg customization

Until we add the feature back...

* ... users will not have a standard API for accelerating the standard `bulk` family of
  algorithms on their own parallel execution contexts (e.g., thread pools). 

* ... hardware vendors like NVIDIA will not be able to ship a scheduler for an exotic
  (non-CPU-ish) execution context and have it work with any standard-conforming
  implementation of std::execution.

This is only a temporary state of affairs. I describe below how the feature can be added
post-C++26 non-intrusively.

# Removal process

The algorithm customization feature can be removed cleanly. Here is what it will entail:

1. Remove the type `default_domain` ([exec.domain.default]{.sref}).

2. Remove the functions:

   * `transform_sender` ([exec.snd.transform]{.sref}),
   * `transform_env` ([exec.snd.transform.env]{.sref}), and
   * `apply_sender` ([exec.snd.apply]{.sref}).

3. Remove the query object `get_domain` ([exec.get.domain]{.sref}).

4. Remove the exposition-only helpers:

   * _`completion-domain`_ ([exec.snd.expos]{.sref}/8-9),
   * _`get-domain-early`_ ([exec.snd.expos]{.sref}/13), and
   * _`get-domain-late`_ ([exec.snd.expos]{.sref}/14).

5. Change the functions `get_completion_signatures` ([exec.getcomplsigs]{.sref}) and
   `connect` ([exec.connect]{.sref}) to operate directly on the sender passed in rather
   than passing the sender through `transform_sender` first.

6. For each sender adaptor algorithm in [exec.adapt]{.sref} that is specified to be
   expression-equivalant to some `transform_sender` invocation of the form:

   > ```c++
   > transform_sender(@_`some-computed-domain`_@(), @_`make-sender`_@(tag, {args...}, sndr));
   > ```

   Change the expression to:

   > ```c++
   > @_`make-sender`_@(tag, {args...}, sndr);
   > ```

   For example, in [exec.continues.on]{.sref}/3, the following:

   > ```c++
   > transform_sender(@_`get-domain-early`_@(sndr), @_`make-sender`_@(continues_on, sch, sndr))
   > ```

   would be changed to:

   > ```c++
   > @_`make-sender`_@(continues_on, sch, sndr)
   > ```

   Additionally, if there is some caveat of the form "except that `sndr` is evaluated only
   once," that caveat should be removed as appropriate.

7. Merge the `schedule_from` ([exec.schedule.from]{.sref}) and `continues_on`
   ([exec.continues.on]{.sref}) algorithms into one algorithm called `continues_on`.
   (Currently they are separate so that they can be customized independently; by default
   `continues_on` merely dispatches to `schedule_from`.)

8. For the following algorithms that are currently expressed in terms of a sender
   transformation to a lowered form, move the lowering from
   `@_alg_@.transform_sender(sndr, env)`{.cpp} to
   `@_impls-for_@<@_alg-t_@>::@_get-state_@(sndr, rcvr)`{.cpp}.

   * `starts_on` ([exec.starts.on]{.sref}),
   * `continues_on` ([exec.continues.on]{.sref}),
   * `on` ([exec.on]{.sref}),
   * `bulk` ([exec.bulk]{.sref}),
   * `when_all_with_variant` ([exec.when.all]{.sref}),
   * `stopped_as_optional` ([exec.stopped.opt]{.sref}), and
   * `stopped_as_error` ([exec.stopped.err]{.sref}).

   `@_impls-for_@<@_alg-t_@>::@_get-state_@`{.cpp} is called when constructing the
   sender's operation state, which happens in `connect`. As currently specified,
   `@_alg_@.transform_sender(sndr, env)`{.cpp} is called from `connect`, so by moving the
   transformation from there to `@_impls-for_@<@_alg-t_@>::@_get-state_@`, there is no
   change of semantics.

9. Change [exec.sync.wait]{.sref} and [exec.sync.wait.var]{.sref} to dispatch directly to
   their default implementations instead of computing a domain and using `apply_sender` to
   dispatch to an implementation.

10. Change [exec.affine.on]{.sref} TODO

11. Tweak the wording of `parallel_scheduler` ([exec.par.scheduler]{.sref}) to indicate
   that it (`parallel_scheduler`) is permitted to run the `bulk` family of algorithms in
   parallel in accordance with those algorithms' semantics, rather than suggesting that
   those algorithms are "customized" for `parallel_scheduler`.

12. Add a new paragraph to [exec.get.scheduler]{.sref} that requires that an operation's
   `start()` function is called on the scheduler obtained from the receiver's environment.
   This codifies something that was already necessarily true: that the receiver's
   scheduler is the "current" scheduler (the one that created the execution agent that is
   currently executing).

This is admittedly a lot of changes, but each of these changes represents a simplification
from the status quo. The result will be a vastly simpler specification for [exec].

# Adding the feature back

Post-C++26, we can add back a way to query a sender/receiver for the domains on which
the async operation will start and complete. Such a query will have a default: the
`default_domain` that represents execution on a standard thread of execution. The query
will be a so-called "forwarding" query, so it will get passed through adaptation layers
automatically without needing to change any senders or receivers.

At this point, we can change `connect`, `get_completion_signatures`, and the sender
algorithms to check for this domain information and use it to find transforms and/or
customizations. This will not affect any senders already in the wild, none of which will
have such a domain query.

# The problem, in examples

[@P3718R0] identifies real problems with the status quo of sender algorithm customization.
It proposes using information from the sender about where it will *complete* during
"early" customization, which happens when a sender algorithm constructs and returns a
sender; and it proposes using information from the receiver about where the operation will
*start* during "late" customization, when the sender and the receiver are connected.

The problem with this separation of responsibilities is that many senders do not know
where they will complete until they know where they will be started. A simple example is
the `just()` sender; it completes inline wherever it is started.

If an algorithm like `then(sndr, fn)` asks `sndr` for where it will complete, `sndr` might
not be able to answer, so no "early" customization is found. And during "late"
(`connect`-time) customization, only the receiver's information about where the operation
will start is used to find a customization. Presumably an algorithm like `then(sndr, fn)`
would want to dispatch based on where the function `fn` will execute, but P3718 does
not make that information generally available.

An illustrative example is:

> ```c++
> auto sndr = starts_on( gpu, just() ) | then( fn );
> sync_wait( std::move(sndr) );
> ```

... where `gpu` is a scheduler that runs work (unsurprisingly) on a GPU.

`fn` will execute on the GPU, so a GPU implementation of `then` should be used. By the
proposed resolution of P3718, algorithm customization proceeds as follows:

* During early customization, when `starts_on( gpu, just() ) | then( fn )`{.cpp} is executing,
  the `then` algorithm asks the `starts_on( gpu, just() )`{.cpp} sender where it will
  complete as if by:

  > ```c++
  > auto&& @_`tmp`_@ = starts_on( gpu, just() );
  > auto @_`d1`_@ = get_domain(get_env(@_`tmp`_@));
  > ```

* The `starts_on` sender will in turn ask the `just()` sender, as if by:
  
  > ```c++
  > auto @_`d2`_@ = get_domain(get_env(just()));
  > ```
  
  As discussed above, the `just()`{.cpp} sender doesn't know where it will complete until
  it knows where it will be started, but that information is not made available to the
  `just()`{.cpp} sender when asking for its completion domain. As a result, _`d2`_ ends up
  as `default_domain`, which is then reported as the domain for the `starts_on` sender.
  That's incorrect. The `starts_on` sender will complete on the GPU.

* The `then` CPO uses `default_domain` to find an implementation of the `then` algorithm,
  which will find the default implementation. As a result, the `then` CPO returns an
  ordinary `then` sender.

* When that `then` sender is connected to `sync_wait`'s receiver, "late" customization
  happens. `connect` asks `sync_wait`'s receiver where the `then` sender will be started.
  It does that with `get_domain(get_env(rcvr))`{.cpp}. `sync_wait` starts operations on
  the current thread, so the `get_domain` query will return `default_domain`. As with
  early customization, late customization will also not find a GPU implementation.

The end result of all of this is that a CPU implementation will be used to evaluate
the `then` algorithm on the GPU. That is a bad state of affairs.


# Proposed resolution

TODO
